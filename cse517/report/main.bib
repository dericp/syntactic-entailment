@ARTICLE{Parikh2016-em,
  title         = "A Decomposable Attention Model for Natural Language
                   Inference",
  author        = "Parikh, Ankur P and T{\"a}ckstr{\"o}m, Oscar and Das,
                   Dipanjan and Uszkoreit, Jakob",
  abstract      = "We propose a simple neural architecture for natural language
                   inference. Our approach uses attention to decompose the
                   problem into subproblems that can be solved separately, thus
                   making it trivially parallelizable. On the Stanford Natural
                   Language Inference (SNLI) dataset, we obtain
                   state-of-the-art results with almost an order of magnitude
                   fewer parameters than previous work and without relying on
                   any word-order information. Adding intra-sentence attention
                   that takes a minimum amount of order into account yields
                   further improvements.",
  month         =  jun,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1606.01933"
}

@ARTICLE{Bowman2015-is,
  title         = "A large annotated corpus for learning natural language
                   inference",
  author        = "Bowman, Samuel R and Angeli, Gabor and Potts, Christopher
                   and Manning, Christopher D",
  abstract      = "Understanding entailment and contradiction is fundamental to
                   understanding natural language, and inference about
                   entailment and contradiction is a valuable testing ground
                   for the development of semantic representations. However,
                   machine learning research in this area has been dramatically
                   limited by the lack of large-scale resources. To address
                   this, we introduce the Stanford Natural Language Inference
                   corpus, a new, freely available collection of labeled
                   sentence pairs, written by humans doing a novel grounded
                   task based on image captioning. At 570K pairs, it is two
                   orders of magnitude larger than all other resources of its
                   type. This increase in scale allows lexicalized classifiers
                   to outperform some sophisticated existing entailment models,
                   and it allows a neural network-based model to perform
                   competitively on natural language inference benchmarks for
                   the first time.",
  month         =  aug,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1508.05326"
}

@ARTICLE{Williams2017-uh,
  title         = "A {Broad-Coverage} Challenge Corpus for Sentence
                   Understanding through Inference",
  author        = "Williams, Adina and Nangia, Nikita and Bowman, Samuel R",
  abstract      = "This paper introduces the Multi-Genre Natural Language
                   Inference (MultiNLI) corpus, a dataset designed for use in
                   the development and evaluation of machine learning models
                   for sentence understanding. In addition to being one of the
                   largest corpora available for the task of NLI, at 433k
                   examples, this corpus improves upon available resources in
                   its coverage: it offers data from ten distinct genres of
                   written and spoken English--making it possible to evaluate
                   systems on nearly the full complexity of the language--and
                   it offers an explicit setting for the evaluation of
                   cross-genre domain adaptation.",
  month         =  apr,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1704.05426"
}

@ARTICLE{Gururangan2018-lj,
  title         = "Annotation Artifacts in Natural Language Inference Data",
  author        = "Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer
                   and Schwartz, Roy and Bowman, Samuel R and Smith, Noah A",
  abstract      = "Large-scale datasets for natural language inference are
                   created by presenting crowd workers with a sentence
                   (premise), and asking them to generate three new sentences
                   (hypotheses) that it entails, contradicts, or is logically
                   neutral with respect to. We show that, in a significant
                   portion of such data, this protocol leaves clues that make
                   it possible to identify the label by looking only at the
                   hypothesis, without observing the premise. Specifically, we
                   show that a simple text categorization model can correctly
                   classify the hypothesis alone in about 67\% of SNLI (Bowman
                   et. al, 2015) and 53\% of MultiNLI (Williams et. al, 2017).
                   Our analysis reveals that specific linguistic phenomena such
                   as negation and vagueness are highly correlated with certain
                   inference classes. Our findings suggest that the success of
                   natural language inference models to date has been
                   overestimated, and that the task remains a hard open
                   problem.",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1803.02324"
}

@INPROCEEDINGS{Khot2018-th,
  title     = "{SciTail}: A textual entailment dataset from science question
               answering",
  booktitle = "Proceedings of {AAAI}",
  author    = "Khot, Tushar and Sabharwal, Ashish and Clark, Peter",
  year      =  2018
}

@ARTICLE{Stern2017-co,
  title         = "A Minimal {Span-Based} Neural Constituency Parser",
  author        = "Stern, Mitchell and Andreas, Jacob and Klein, Dan",
  abstract      = "In this work, we present a minimal neural model for
                   constituency parsing based on independent scoring of labels
                   and spans. We show that this model is not only compatible
                   with classical dynamic programming techniques, but also
                   admits a novel greedy top-down inference algorithm based on
                   recursive partitioning of the input. We demonstrate
                   empirically that both prediction schemes are competitive
                   with recent work, and when combined with basic extensions to
                   the scoring model are capable of achieving state-of-the-art
                   single-model performance on the Penn Treebank (91.79 F1) and
                   strong performance on the French Treebank (82.23 F1).",
  month         =  may,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1705.03919"
}

@ARTICLE{Dozat2016-gs,
  title         = "Deep Biaffine Attention for Neural Dependency Parsing",
  author        = "Dozat, Timothy and Manning, Christopher D",
  abstract      = "This paper builds off recent work from Kiperwasser \&
                   Goldberg (2016) using neural attention in a simple
                   graph-based dependency parser. We use a larger but more
                   thoroughly regularized parser than other recent BiLSTM-based
                   approaches, with biaffine classifiers to predict arcs and
                   labels. Our parser gets state of the art or near state of
                   the art performance on standard treebanks for six different
                   languages, achieving 95.7\% UAS and 94.1\% LAS on the most
                   popular English PTB dataset. This makes it the
                   highest-performing graph-based parser on this
                   benchmark---outperforming Kiperwasser Goldberg (2016) by
                   1.8\% and 2.2\%---and comparable to the highest performing
                   transition-based parser (Kuncoro et al., 2016), which
                   achieves 95.8\% UAS and 94.6\% LAS. We also show which
                   hyperparameter choices had a significant effect on parsing
                   accuracy, allowing us to achieve large gains over other
                   graph-based approaches.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1611.01734"
}

@ARTICLE{Peters2018-fz,
  title    = "Deep contextualized word representations",
  author   = "Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner,
              Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke",
  abstract = "We introduce a new type of deep contextualized word
              representation that models both (1) complex characteristics of
              word use (e.g., syntax and semantics), and (2) how these uses
              vary across linguistic contexts (i.e., to model polysemy). Our
              word vectors are learned functions of the internal states of a
              deep bidirectional language model (biLM), which is pre-trained on
              a large text corpus. We show that these representations can be
              easily added to existing models and significantly improve the
              state of the art across six challenging NLP problems, including
              question answering, textual entailment and sentiment analysis. We
              also present an analysis showing that exposing the deep internals
              of the pre-trained network is crucial, allowing downstream models
              to mix different types of semi-supervision signals.",
  journal  = "arXiv:1802.05365 [cs]",
  month    =  feb,
  year     =  2018
}

@ARTICLE{Devlin2018-qc,
  title         = "{BERT}: Pre-training of Deep Bidirectional Transformers for
                   Language Understanding",
  author        = "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and
                   Toutanova, Kristina",
  abstract      = "We introduce a new language representation model called
                   BERT, which stands for Bidirectional Encoder Representations
                   from Transformers. Unlike recent language representation
                   models, BERT is designed to pre-train deep bidirectional
                   representations by jointly conditioning on both left and
                   right context in all layers. As a result, the pre-trained
                   BERT representations can be fine-tuned with just one
                   additional output layer to create state-of-the-art models
                   for a wide range of tasks, such as question answering and
                   language inference, without substantial task-specific
                   architecture modifications. BERT is conceptually simple and
                   empirically powerful. It obtains new state-of-the-art
                   results on eleven natural language processing tasks,
                   including pushing the GLUE benchmark to 80.4\% (7.6\%
                   absolute improvement), MultiNLI accuracy to 86.7 (5.6\%
                   absolute improvement) and the SQuAD v1.1 question answering
                   Test F1 to 93.2 (1.5\% absolute improvement), outperforming
                   human performance by 2.0\%.",
  month         =  oct,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1810.04805"
}

@unpublished{Pang2018-syntail,
    title= {Syntail: Syntactically-Informed Natural Language Inference},
    author = {Pang, Deric and Liu, Lynsey and Johnston, Aaron},
    year = {2018},
    note= {CSE 599 G1 Deep Learning},
    URL= {https://docs.google.com/presentation/d/1FjLbETb7Y5byfGvN3OOHWQAmR6qNnYaANWFANiMy1O4/edit?usp=sharing}
}

@article{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  year={1993}
}

@ARTICLE{Chen2016-wl,
  title         = "Enhanced {LSTM} for Natural Language Inference",
  author        = "Chen, Qian and Zhu, Xiaodan and Ling, Zhenhua and Wei, Si
                   and Jiang, Hui and Inkpen, Diana",
  abstract      = "Reasoning and inference are central to human and artificial
                   intelligence. Modeling inference in human language is very
                   challenging. With the availability of large annotated data
                   (Bowman et al., 2015), it has recently become feasible to
                   train neural network based inference models, which have
                   shown to be very effective. In this paper, we present a new
                   state-of-the-art result, achieving the accuracy of 88.6\% on
                   the Stanford Natural Language Inference Dataset. Unlike the
                   previous top models that use very complicated network
                   architectures, we first demonstrate that carefully designing
                   sequential inference models based on chain LSTMs can
                   outperform all previous models. Based on this, we further
                   show that by explicitly considering recursive architectures
                   in both local inference modeling and inference composition,
                   we achieve additional improvement. Particularly,
                   incorporating syntactic parsing information contributes to
                   our best result---it further improves the performance even
                   when added to the already very strong model.",
  month         =  sep,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1609.06038"
}

@INPROCEEDINGS{Pennington2014-uo,
  title     = "Glove: Global vectors for word representation",
  booktitle = "Proceedings of the 2014 conference on empirical methods in
               natural language processing ({EMNLP})",
  author    = "Pennington, Jeffrey and Socher, Richard and Manning, Christopher",
  abstract  = "… The skip-gram and continuous bag-of- words (CBOW) models of
               Mikolov et al … 3 The GloVe Model The statistics of word
               occurrences in a corpus is the primary source of information
               available to all unsupervised methods for learning word
               represen- tations, and although many …",
  publisher = "aclweb.org",
  pages     = "1532--1543",
  year      =  2014
}

@ARTICLE{Swayamdipta2017-ct,
  title         = "{Frame-Semantic} Parsing with {Softmax-Margin} Segmental
                   {RNNs} and a Syntactic Scaffold",
  author        = "Swayamdipta, Swabha and Thomson, Sam and Dyer, Chris and
                   Smith, Noah A",
  abstract      = "We present a new, efficient frame-semantic parser that
                   labels semantic arguments to FrameNet predicates. Built
                   using an extension to the segmental RNN that emphasizes
                   recall, our basic system achieves competitive performance
                   without any calls to a syntactic parser. We then introduce a
                   method that uses phrase-syntactic annotations from the Penn
                   Treebank during training only, through a multitask
                   objective; no parsing is required at training or test time.
                   This ``syntactic scaffold'' offers a cheaper alternative to
                   traditional syntactic pipelining, and achieves
                   state-of-the-art performance.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1706.09528"
}

@ARTICLE{Strubell2018-qw,
  title         = "{Linguistically-Informed} {Self-Attention} for Semantic Role
                   Labeling",
  author        = "Strubell, Emma and Verga, Patrick and Andor, Daniel and
                   Weiss, David and McCallum, Andrew",
  abstract      = "Current state-of-the-art semantic role labeling (SRL) uses a
                   deep neural network with no explicit linguistic features.
                   However, prior work has shown that gold syntax trees can
                   dramatically improve SRL decoding, suggesting the
                   possibility of increased accuracy from explicit modeling of
                   syntax. In this work, we present linguistically-informed
                   self-attention (LISA): a neural network model that combines
                   multi-head self-attention with multi-task learning across
                   dependency parsing, part-of-speech tagging, predicate
                   detection and SRL. Unlike previous models which require
                   significant pre-processing to prepare linguistic features,
                   LISA can incorporate syntax using merely raw tokens as
                   input, encoding the sequence only once to simultaneously
                   perform parsing, predicate detection and role labeling for
                   all predicates. Syntax is incorporated by training one
                   attention head to attend to syntactic parents for each
                   token. Moreover, if a high-quality syntactic parse is
                   already available, it can be beneficially injected at test
                   time without re-training our SRL model. In experiments on
                   CoNLL-2005 SRL, LISA achieves new state-of-the-art
                   performance for a model using predicted predicates and
                   standard word embeddings, attaining 2.5 F1 absolute higher
                   than the previous state-of-the-art on newswire and more than
                   3.5 F1 on out-of-domain data, nearly 10\% reduction in
                   error. On ConLL-2012 English SRL we also show an improvement
                   of more than 2.5 F1. LISA also out-performs the
                   state-of-the-art with contextually-encoded (ELMo) word
                   representations, by nearly 1.0 F1 on news and more than 2.0
                   F1 on out-of-domain text.",
  month         =  apr,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1804.08199"
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}